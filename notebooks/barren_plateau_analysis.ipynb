{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Barren Plateaus in Parameterized Quantum Circuits\n",
    "\n",
    "Barren plateaus are one of the biggest challenges for scaling variational quantum algorithms. When the gradient of the cost function vanishes exponentially with the number of qubits, the optimization landscape becomes essentially flat — and no classical optimizer can find a good direction.\n",
    "\n",
    "In this notebook, we empirically investigate:\n",
    "1. How gradient variance scales with circuit width (number of qubits)\n",
    "2. How circuit depth affects trainability\n",
    "3. Whether global vs local cost functions make a difference\n",
    "4. Strategies for avoiding barren plateaus: initialization, layer-wise training\n",
    "\n",
    "We use the **parameter shift rule** to compute exact analytical gradients, averaged over many random initializations to estimate the gradient variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "from src.circuits import (\n",
    "    build_hardware_efficient_ansatz, build_structured_ansatz,\n",
    "    build_identity_block_ansatz, get_ansatz_builders\n",
    ")\n",
    "from src.gradients import (\n",
    "    compute_gradient_parameter_shift, compute_gradient_variance,\n",
    "    sweep_gradient_variance, sweep_depth_variance\n",
    ")\n",
    "from src.cost_functions import (\n",
    "    global_cost_observable, local_cost_observable,\n",
    "    two_local_cost_observable\n",
    ")\n",
    "from src.trainability import (\n",
    "    layerwise_train, random_init, correlated_init, identity_init\n",
    ")\n",
    "from src.plotting import (\n",
    "    plot_gradient_variance_vs_qubits, plot_gradient_variance_vs_depth,\n",
    "    plot_global_vs_local_cost, plot_cost_landscape_1d,\n",
    "    plot_variance_heatmap, plot_layerwise_convergence,\n",
    "    plot_ansatz_comparison\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "RESULTS_DIR = Path('../results')\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print('Setup complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Parameter Shift Rule\n",
    "\n",
    "Before we measure gradients, let's understand how. The **parameter shift rule** gives us exact analytical gradients for parameterized quantum circuits:\n",
    "\n",
    "$$\\frac{\\partial \\langle C \\rangle}{\\partial \\theta_k} = \\frac{1}{2}\\left[\\langle C \\rangle_{\\theta_k + \\pi/2} - \\langle C \\rangle_{\\theta_k - \\pi/2}\\right]$$\n",
    "\n",
    "We evaluate the circuit at two shifted parameter values and take the difference. No finite-difference approximation needed.\n",
    "\n",
    "Let's verify this works on a small circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick sanity check: gradient of a simple circuit\n",
    "circuit = build_hardware_efficient_ansatz(n_qubits=2, depth=1)\n",
    "observable = global_cost_observable(n_qubits=2)\n",
    "\n",
    "print(f\"Circuit has {circuit.num_parameters} parameters\")\n",
    "print(f\"Circuit depth: {circuit.depth()}\")\n",
    "print()\n",
    "\n",
    "# compute gradient at a random point\n",
    "rng = np.random.default_rng(42)\n",
    "params = rng.uniform(0, 2 * np.pi, circuit.num_parameters)\n",
    "\n",
    "print(\"Gradients at a random parameter point:\")\n",
    "for i in range(min(4, circuit.num_parameters)):\n",
    "    grad = compute_gradient_parameter_shift(\n",
    "        circuit, observable, params, param_index=i\n",
    "    )\n",
    "    print(f\"  d<C>/d(theta_{i}) = {grad:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiment 1: Gradient Variance vs Number of Qubits\n",
    "\n",
    "This is the core barren plateau experiment. We measure how the variance of the gradient changes as we add more qubits to the circuit.\n",
    "\n",
    "**Theory predicts**: For a hardware-efficient ansatz with a global cost function, the gradient variance should decay *exponentially* with the number of qubits:\n",
    "\n",
    "$$\\text{Var}\\left[\\frac{\\partial C}{\\partial \\theta_k}\\right] \\sim \\mathcal{O}\\left(\\frac{1}{2^n}\\right)$$\n",
    "\n",
    "This means a 10-qubit circuit has ~1000x smaller gradients than a 2-qubit circuit. Let's see if this holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweep qubit count with hardware-efficient ansatz + global cost\n",
    "qubit_range = [2, 4, 6, 8, 10, 12]\n",
    "depth = 4  # fixed depth\n",
    "n_samples = 200  # random initializations per configuration\n",
    "\n",
    "print(f\"Sweeping qubits {qubit_range} with depth={depth}, {n_samples} samples each\")\n",
    "print(\"This takes a few minutes...\\n\")\n",
    "\n",
    "hw_global_results = sweep_gradient_variance(\n",
    "    qubit_range=qubit_range,\n",
    "    depth=depth,\n",
    "    ansatz_builder=build_hardware_efficient_ansatz,\n",
    "    observable_builder=global_cost_observable,\n",
    "    n_samples=n_samples\n",
    ")\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "for n, var in zip(qubit_range, hw_global_results['variances']):\n",
    "    print(f\"  {n} qubits: variance = {var:.6e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot on log scale\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "variances = hw_global_results['variances']\n",
    "ax.semilogy(qubit_range, variances, 'o-', color='#FF6B6B', linewidth=2,\n",
    "           markersize=10, label='Hardware-efficient + Global cost')\n",
    "\n",
    "# fit exponential decay line for reference\n",
    "log_vars = np.log(variances)\n",
    "coeffs = np.polyfit(qubit_range, log_vars, 1)\n",
    "fit_line = np.exp(np.polyval(coeffs, qubit_range))\n",
    "ax.semilogy(qubit_range, fit_line, '--', color='gray', linewidth=1.5,\n",
    "           label=f'Exponential fit (slope={coeffs[0]:.2f})', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Number of Qubits')\n",
    "ax.set_ylabel('Gradient Variance')\n",
    "ax.set_title('Barren Plateau: Gradient Variance vs Circuit Width')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'variance_vs_qubits.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Exponential decay rate: {coeffs[0]:.3f} per qubit\")\n",
    "print(f\"This means each added qubit reduces variance by ~{np.exp(coeffs[0]):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiment 2: Global vs Local Cost Function\n",
    "\n",
    "Cerezo et al. (2021) showed that the choice of cost function matters. **Global cost functions** (that measure all qubits) lead to barren plateaus even for shallow circuits, while **local cost functions** (that measure only a few qubits) can avoid them for logarithmic-depth circuits.\n",
    "\n",
    "Let's test this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same sweep but with local cost function\n",
    "print(\"Sweeping with LOCAL cost function...\")\n",
    "hw_local_results = sweep_gradient_variance(\n",
    "    qubit_range=qubit_range,\n",
    "    depth=depth,\n",
    "    ansatz_builder=build_hardware_efficient_ansatz,\n",
    "    observable_builder=local_cost_observable,\n",
    "    n_samples=n_samples\n",
    ")\n",
    "\n",
    "# compare\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.semilogy(qubit_range, hw_global_results['variances'], 'o-', color='#FF6B6B',\n",
    "           linewidth=2, markersize=8, label='Global cost (sum Z_i)')\n",
    "ax.semilogy(qubit_range, hw_local_results['variances'], 's-', color='#4ECDC4',\n",
    "           linewidth=2, markersize=8, label='Local cost (Z_0 only)')\n",
    "\n",
    "ax.set_xlabel('Number of Qubits')\n",
    "ax.set_ylabel('Gradient Variance')\n",
    "ax.set_title('Global vs Local Cost Function (depth=4)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'global_vs_local.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nComparison:\")\n",
    "for n, g_var, l_var in zip(qubit_range, hw_global_results['variances'],\n",
    "                           hw_local_results['variances']):\n",
    "    ratio = l_var / g_var if g_var > 0 else float('inf')\n",
    "    print(f\"  {n} qubits: global={g_var:.2e}, local={l_var:.2e}, ratio={ratio:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiment 3: Gradient Variance vs Circuit Depth\n",
    "\n",
    "Deeper circuits are more expressive but potentially less trainable. Let's fix the number of qubits and sweep the depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweep depth at fixed qubit count\n",
    "fixed_qubits = 6\n",
    "depth_range = [1, 2, 4, 8, 12, 16]\n",
    "\n",
    "print(f\"Sweeping depth {depth_range} at {fixed_qubits} qubits...\")\n",
    "depth_results = sweep_depth_variance(\n",
    "    n_qubits=fixed_qubits,\n",
    "    depth_range=depth_range,\n",
    "    ansatz_builder=build_hardware_efficient_ansatz,\n",
    "    observable_builder=global_cost_observable,\n",
    "    n_samples=n_samples\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.semilogy(depth_range, depth_results['variances'], 'o-', color='#45B7D1',\n",
    "           linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Circuit Depth (layers)')\n",
    "ax.set_ylabel('Gradient Variance')\n",
    "ax.set_title(f'Gradient Variance vs Circuit Depth ({fixed_qubits} qubits, global cost)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'variance_vs_depth.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment 4: Ansatz Architecture Comparison\n",
    "\n",
    "Not all circuit architectures are equally prone to barren plateaus. We compare:\n",
    "\n",
    "- **Hardware-efficient**: full Ry/Rz rotations + linear CNOT entanglement (standard, known to have BPs)\n",
    "- **Structured**: limited entanglement, fewer parameters (designed to be more trainable)\n",
    "- **Identity-block**: initialized near identity (Grant et al. 2019 strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare three ansatz architectures\n",
    "ansatz_builders = get_ansatz_builders()\n",
    "ansatz_results = {}\n",
    "\n",
    "for name, builder in ansatz_builders.items():\n",
    "    print(f\"\\nSweeping {name}...\")\n",
    "    results = sweep_gradient_variance(\n",
    "        qubit_range=qubit_range,\n",
    "        depth=4,\n",
    "        ansatz_builder=builder,\n",
    "        observable_builder=global_cost_observable,\n",
    "        n_samples=n_samples\n",
    "    )\n",
    "    ansatz_results[name] = results\n",
    "\n",
    "# plot comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors_ansatz = {'Hardware-Efficient': '#FF6B6B', 'Structured': '#4ECDC4',\n",
    "                 'Identity-Block': '#45B7D1'}\n",
    "markers = {'Hardware-Efficient': 'o', 'Structured': 's', 'Identity-Block': '^'}\n",
    "\n",
    "for name, results in ansatz_results.items():\n",
    "    ax.semilogy(qubit_range, results['variances'],\n",
    "               marker=markers.get(name, 'o'), color=colors_ansatz.get(name, 'gray'),\n",
    "               linewidth=2, markersize=8, label=name)\n",
    "\n",
    "ax.set_xlabel('Number of Qubits')\n",
    "ax.set_ylabel('Gradient Variance')\n",
    "ax.set_title('Ansatz Architecture Comparison (depth=4, global cost)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'ansatz_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Variance Heatmap\n",
    "\n",
    "Let's build a 2D picture: gradient variance as a function of both width AND depth. This gives a complete map of the trainability landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute variance for a grid of (width, depth) pairs\n",
    "width_values = [2, 4, 6, 8, 10]\n",
    "depth_values = [1, 2, 4, 8, 12]\n",
    "\n",
    "variance_matrix = np.zeros((len(depth_values), len(width_values)))\n",
    "\n",
    "for i, d in enumerate(depth_values):\n",
    "    for j, n in enumerate(width_values):\n",
    "        print(f\"  n={n}, depth={d}...\", end=\" \")\n",
    "        circuit = build_hardware_efficient_ansatz(n, d)\n",
    "        observable = global_cost_observable(n)\n",
    "        var = compute_gradient_variance(\n",
    "            circuit, observable, param_index=0, n_samples=100\n",
    "        )\n",
    "        variance_matrix[i, j] = var\n",
    "        print(f\"var={var:.2e}\")\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "log_variance = np.log10(variance_matrix + 1e-10)\n",
    "im = sns.heatmap(log_variance, annot=True, fmt='.1f',\n",
    "                xticklabels=width_values, yticklabels=depth_values,\n",
    "                cmap='RdYlGn_r', ax=ax, cbar_kws={'label': 'log10(Variance)'})\n",
    "\n",
    "ax.set_xlabel('Number of Qubits')\n",
    "ax.set_ylabel('Circuit Depth')\n",
    "ax.set_title('Gradient Variance Heatmap (Hardware-Efficient, Global Cost)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'variance_heatmap.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Green = trainable, Red = barren plateau\")\n",
    "print(\"The bottom-right corner (wide + deep) is the danger zone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cost Landscape Visualization\n",
    "\n",
    "To build more intuition, let's visualize 1D slices through the cost landscape. For a trainable circuit, we should see clear structure (hills and valleys). For a barren plateau, it should look flat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D cost landscape slices for small vs large circuits\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, (n_q, title) in enumerate([(2, '2 qubits (trainable)'),\n",
    "                                     (10, '10 qubits (barren plateau)')]):\n",
    "    circuit = build_hardware_efficient_ansatz(n_q, depth=4)\n",
    "    observable = global_cost_observable(n_q)\n",
    "    \n",
    "    # fix all params except one, sweep that one from 0 to 2pi\n",
    "    rng = np.random.default_rng(42)\n",
    "    base_params = rng.uniform(0, 2 * np.pi, circuit.num_parameters)\n",
    "    \n",
    "    theta_range = np.linspace(0, 2 * np.pi, 100)\n",
    "    costs = []\n",
    "    \n",
    "    from qiskit.quantum_info import Statevector, SparsePauliOp\n",
    "    \n",
    "    for theta in theta_range:\n",
    "        params = base_params.copy()\n",
    "        params[0] = theta\n",
    "        \n",
    "        bound_circuit = circuit.assign_parameters(params)\n",
    "        sv = Statevector(bound_circuit)\n",
    "        cost = sv.expectation_value(observable).real\n",
    "        costs.append(cost)\n",
    "    \n",
    "    axes[idx].plot(theta_range, costs, linewidth=2, color='#2196F3')\n",
    "    axes[idx].set_xlabel('θ₀')\n",
    "    axes[idx].set_ylabel('⟨C⟩')\n",
    "    axes[idx].set_title(title)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # show variance of this slice\n",
    "    cost_var = np.var(costs)\n",
    "    axes[idx].annotate(f'Cost variance: {cost_var:.4f}',\n",
    "                      xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "                      fontsize=10, verticalalignment='top',\n",
    "                      bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.suptitle('1D Cost Landscape Slices', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'cost_landscape.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Notice how the 10-qubit landscape is much flatter — harder to optimize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Layer-wise Training Strategy\n",
    "\n",
    "One proposed mitigation strategy is **layer-wise training**: instead of optimizing all parameters of a deep circuit at once (which hits the barren plateau), build the circuit one layer at a time:\n",
    "\n",
    "1. Start with 1 layer, optimize those parameters\n",
    "2. Freeze them, add a second layer, optimize the new parameters\n",
    "3. Repeat until you reach the desired depth\n",
    "\n",
    "This way, you never optimize in the full exponentially-flat landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer-wise training on 8 qubits\n",
    "n_q = 8\n",
    "max_layers = 6\n",
    "observable = global_cost_observable(n_q)\n",
    "\n",
    "print(f\"Layer-wise training: {n_q} qubits, up to {max_layers} layers\")\n",
    "layer_histories = layerwise_train(\n",
    "    ansatz_builder=build_hardware_efficient_ansatz,\n",
    "    cost_observable=observable,\n",
    "    n_qubits=n_q,\n",
    "    max_layers=max_layers,\n",
    "    maxiter=50,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# plot convergence for each layer\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors_lw = plt.cm.viridis(np.linspace(0.1, 0.9, max_layers))\n",
    "offset = 0\n",
    "\n",
    "for i, history in enumerate(layer_histories):\n",
    "    iters = range(offset, offset + len(history))\n",
    "    ax.plot(iters, history, color=colors_lw[i], linewidth=1.5,\n",
    "            label=f'Layer {i+1}')\n",
    "    ax.axvline(x=offset, color='gray', linestyle=':', alpha=0.3)\n",
    "    offset += len(history)\n",
    "\n",
    "ax.set_xlabel('Total Iterations')\n",
    "ax.set_ylabel('Cost')\n",
    "ax.set_title(f'Layer-wise Training Convergence ({n_q} qubits)')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'layerwise_convergence.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What we confirmed:\n",
    "\n",
    "1. **Barren plateaus are real** — gradient variance decays exponentially with qubit count for hardware-efficient ansatze with global cost functions, exactly as predicted by McClean et al. (2018).\n",
    "\n",
    "2. **Local cost functions help significantly** — switching from a global to local observable increases gradient variance by orders of magnitude, consistent with Cerezo et al. (2021). This is probably the single most impactful design choice.\n",
    "\n",
    "3. **Circuit architecture matters** — structured ansatze with limited entanglement show slower variance decay than fully random hardware-efficient designs. Identity-block initialization also helps preserve gradient information.\n",
    "\n",
    "4. **Layer-wise training works** — by building the circuit incrementally, we avoid the worst of the barren plateau problem. Each layer starts from a reasonable initial point.\n",
    "\n",
    "5. **Width AND depth both contribute** — the heatmap shows that both wider and deeper circuits have worse trainability, but width has the stronger effect.\n",
    "\n",
    "### Implications for QML algorithm design:\n",
    "\n",
    "- Use **local cost functions** whenever possible\n",
    "- Prefer **shallow circuits** with problem-aware structure over deep random circuits\n",
    "- Use **layer-wise or incremental training** for deeper circuits\n",
    "- **Initialize carefully** — random initialization in wide circuits almost guarantees you start in a barren plateau\n",
    "- The barren plateau problem sets a practical limit on the scalability of current variational QML approaches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
